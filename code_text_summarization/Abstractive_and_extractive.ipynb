{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1954114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Evaluation matrices, rogue score etc\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from gensim import corpora, models\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "# Abstractive Summarization with Seq2Seq\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025090b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "df = pd.read_csv(\"./Text_Summarization/BigBangTheory.csv\", encoding='latin-1')\n",
    "\n",
    "# Assuming your text corpus is in a column named \"text\"\n",
    "text_corpus = df[\"document\"].tolist()\n",
    "#text_corpus = df[\"document\"].str.encode('utf-8')\n",
    "\n",
    "# Custom stop words\n",
    "custom_stop_words = {\"$\", \"£\", \"€\"}  # Define additional stop words as needed\n",
    "\n",
    "# Custom tokenizer to handle compound words\n",
    "tokenizer = Tokenizer(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ee55ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3662bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_annotate(corpus):\n",
    "    annotated_corpus = []\n",
    "    for text in corpus:\n",
    "        # Tokenize text with custom tokenizer to handle compound words\n",
    "        doc = tokenizer(text)\n",
    "        \n",
    "        # Remove stop words, lemmatize, and extract POS tags\n",
    "        tokens = []\n",
    "        pos_tags = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in STOP_WORDS.union(custom_stop_words) and token.text not in punctuation:\n",
    "                tokens.append((token.text, token.lemma_))\n",
    "                pos_tags.append((token.text, token.pos_))\n",
    "        \n",
    "        # Extract Named Entities\n",
    "        entities = [(ent.text, ent.label_) for ent in nlp(text).ents]\n",
    "        \n",
    "        # Word embeddings\n",
    "        word_embeddings = np.array([token.vector for token in doc if not token.is_stop])\n",
    "        \n",
    "        annotated_corpus.append({\n",
    "            'text': text,\n",
    "            'tokens': tokens,\n",
    "            'pos_tags': pos_tags,\n",
    "            'entities': entities,\n",
    "            'word_embeddings' : word_embeddings\n",
    "        })\n",
    "    return annotated_corpus\n",
    "\n",
    "preprocessed_corpus = []\n",
    "\n",
    "# Preprocess and annotate text corpus\n",
    "annotated_corpus = preprocess_and_annotate(text_corpus)\n",
    "text_filtered = []\n",
    "for annotation in annotated_corpus:\n",
    "    y=[]\n",
    "    y.append(annotation['tokens'])\n",
    "    for x in annotation['tokens']:\n",
    "        #print(x[0])\n",
    "        preprocessed_corpus.append(x[0])\n",
    "        \n",
    "    text_filtered.append(preprocessed_corpus)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c84c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Topic Modeling with LDA\n",
    "dictionary = corpora.Dictionary(text_filtered)\n",
    "#corpus = [dictionary.doc2bow(text.split()) for text in text_filtered]\n",
    "corpus = [dictionary.doc2bow(text) for text in text_filtered]\n",
    "lda_model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016513e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.062*\" \" + 0.026*\"Big\" + 0.023*\"Bang\" + 0.017*\"universe\" + 0.013*\"cosmic\" + 0.011*\"years\" + 0.011*\"models\" + 0.011*\"\" + 0.010*\"billion\" + 0.008*\"expansion\"\n",
      "Topic: 1 \n",
      "Words: 0.043*\" \" + 0.020*\"Big\" + 0.017*\"Bang\" + 0.015*\"universe\" + 0.010*\"years\" + 0.009*\"models\" + 0.009*\"\" + 0.008*\"cosmic\" + 0.008*\"known\" + 0.007*\"light\"\n",
      "Topic: 2 \n",
      "Words: 0.075*\" \" + 0.025*\"universe\" + 0.023*\"Bang\" + 0.020*\"Big\" + 0.013*\"cosmic\" + 0.013*\"billion\" + 0.010*\"\" + 0.010*\"years\" + 0.010*\"models\" + 0.009*\"known\"\n",
      "Topic: 3 \n",
      "Words: 0.091*\" \" + 0.028*\"Big\" + 0.019*\"universe\" + 0.018*\"Bang\" + 0.012*\"cosmic\" + 0.011*\"\" + 0.010*\"models\" + 0.010*\"years\" + 0.009*\"expansion\" + 0.009*\"cosmological\"\n",
      "Topic: 4 \n",
      "Words: 0.050*\" \" + 0.021*\"Big\" + 0.018*\"Bang\" + 0.017*\"universe\" + 0.011*\"cosmic\" + 0.011*\"models\" + 0.009*\"years\" + 0.009*\"billion\" + 0.008*\"\" + 0.007*\"expansion\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec28c6",
   "metadata": {},
   "source": [
    "T5-Small: 60 million parameters 242 mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842e983",
   "metadata": {},
   "source": [
    "T5-Base: 220 million parameters 892 mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb44da",
   "metadata": {},
   "source": [
    "T5-Large: 770 million parameters 2.95 gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86d5bf",
   "metadata": {},
   "source": [
    "T5-3B: 3 billion parameters 11.4 gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10706cf1",
   "metadata": {},
   "source": [
    "T5-11B: 11 billion parameters 45.2 gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "274d6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type =\"t5-large\" \n",
    "\n",
    "# Set the model_max_length parameter to a value suitable for your use case\n",
    "model_max_length = 512\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_type)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_type, model_max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ef49f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.'], ['Big', 'Bang', 'Theory', 'leading', 'explanation', 'universe', 'began.', 'Simply', 'put,', 'says', 'universe', 'know', 'started', 'infinitely', 'hot', 'dense', 'single', 'point', 'inflated', 'stretched', '\\x97', 'unimaginable', 'speeds,', 'measurable', 'rate', '\\x97', '13.7', 'billion', 'years', 'still-expanding', 'cosmos', 'know', 'today.Crucially,', 'models', 'compatible', '\\xa0', 'Hubble\\x96Lemaître', 'law\\x97the', 'observation', 'farther', 'away', '\\xa0', 'galaxy', '\\xa0', 'is,', 'faster', 'moving', 'away', 'Earth.', 'Extrapolating', '\\xa0', 'cosmic', 'expansion', '\\xa0', 'backwards', 'time', 'known', '\\xa0', 'laws', 'physics,', 'models', 'describe', 'increasingly', 'concentrated', 'cosmos', 'preceded', '\\xa0', 'singularity', '\\xa0', '\\xa0', 'space', 'time', '\\xa0', 'lose', 'meaning', '(typically', 'named', '\"the', 'Big', 'Bang', 'singularity\").[5]', '\\xa0', '1964', 'CMB', 'discovered,', 'convinced', 'cosmologists', 'competing', '\\xa0', 'steady-state', 'model', '\\xa0', 'cosmic', 'evolution', '\\xa0', 'falsified,[6]', '\\xa0', 'Big', 'Bang', 'models', 'predict', 'uniform', 'background', 'radiation', 'caused', 'high', 'temperatures', 'densities', 'distant', 'past.', 'wide', 'range', 'empirical', 'evidence', 'strongly', 'favors', 'Big', 'Bang', 'event,', 'essentially', 'universally', 'accepted.[7]', '\\xa0', 'Detailed', 'measurements', 'expansion', 'rate', '\\xa0', 'universe', '\\xa0', 'place', 'Big', 'Bang', 'singularity', 'estimated', '\\xa0', '13.787±0.020', '\\xa0', 'billion', '\\xa0', 'years', 'ago,', 'considered', '\\xa0', 'age', 'universe.[8]', 'Existing', 'technology', \"doesn't\", 'allow', 'astronomers', 'literally', 'peer', \"universe's\", 'birth,', 'understand', 'Big', 'Bang', 'comes', 'mathematical', 'formulas', 'models.', 'Astronomers', 'can,', 'however,', '\"echo\"', 'expansion', 'phenomenon', 'known', 'cosmic', 'microwave', 'background.The', 'Big', 'Bang', 'models', 'offer', 'comprehensive', 'explanation', 'broad', 'range', 'observed', 'phenomena,', 'including', 'abundances', '\\xa0', 'light', 'elements,', '\\xa0', 'CMB,', '\\xa0', 'large-scale', 'structure,', '\\xa0', \"Hubble's\", 'law.[10]', '\\xa0', 'models', 'depend', 'major', 'assumptions:', 'universality', 'physical', 'laws', '\\xa0', 'cosmological', 'principle.', 'universality', 'physical', 'laws', 'underlying', 'principles', '\\xa0', 'theory', 'relativity.', 'cosmological', 'principle', 'states', 'large', 'scales', '\\xa0', 'universe', '\\xa0', '\\xa0', 'homogeneous', '\\xa0', '\\xa0', 'isotropic\\x97appearing', 'directions', 'regardless', 'location.', 'majority', 'astronomical', 'community', 'accepts', 'theory,', 'theorists', 'alternative', 'explanations', 'Big', 'Bang', '\\x97', 'eternal', 'inflation', 'oscillating', 'universe.The', 'large-scale', 'universe', 'appears', 'isotropic', 'viewed', 'Earth.', 'isotropic,', 'cosmological', 'principle', 'derived', 'simpler', '\\xa0', 'Copernican', 'principle,', 'states', 'preferred', '(or', 'special)', 'observer', 'vantage', 'point.', 'end,', 'cosmological', 'principle', 'confirmed', 'level', '10?5', '\\xa0', 'observations', 'temperature', 'CMB.', 'scale', 'CMB', 'horizon,', 'universe', 'measured', 'homogeneous', 'upper', 'bound', '\\xa0', 'order', '\\xa0', '10%', 'inhomogeneity,', '1995.', '13.7', 'billion', 'years', 'ago,', 'entire', 'universe', 'condensed', 'infinitesimally', 'small', 'singularity,', 'point', 'infinite', 'denseness', 'heat.', 'Suddenly,', 'explosive', 'expansion', 'began,', 'ballooning', 'universe', 'outwards', 'faster', 'speed', 'light.', 'period', 'cosmic', 'inflation', 'lasted', 'mere', 'fractions', 'second', '\\x97', '10^-32', 'second,', 'according', 'physicist', 'Alan', 'Guth\\x92s', '1980', 'theory', 'changed', 'way', 'think', 'Big', 'Bang', 'forever.', 'cosmic', 'inflation', 'came', 'sudden', 'still-mysterious', 'end,', 'classic', 'descriptions', 'Big', 'Bang', 'took', 'hold.', 'flood', 'matter', 'radiation,', 'known', '\"reheating,\"', 'began', 'populating', 'universe', 'stuff', 'know', 'today:', 'particles,', 'atoms,', 'stuff', 'stars', 'galaxies', 'on.', 'happened', 'second', 'universe', 'began,', 'temperature', 'insanely', 'hot,', '10', 'billion', 'degrees', 'Fahrenheit', '(5.5', 'billion', 'Celsius),', 'according', 'NASA.', 'cosmos', 'contained', 'vast', 'array', 'fundamental', 'particles', 'neutrons,', 'electrons', 'protons', '\\x97', 'raw', 'materials', 'building', 'blocks', 'exists', 'today.', 'early', '\"soup\"', 'impossible', 'actually', \"couldn't\", 'hold', 'visible', 'light.', '\"The', 'free', 'electrons', 'caused', 'light', '(photons)', 'scatter', 'way', 'sunlight', 'scatters', 'water', 'droplets', 'clouds,\"', 'NASA', 'stated.', 'time,', 'however,', 'free', 'electrons', 'met', 'nuclei', 'created', 'neutral', 'atoms', 'atoms', 'equal', 'positive', 'negative', 'electric', 'charges.', 'allowed', 'light', 'finally', 'shine', 'through,', '380,000', 'years', 'Big', 'Bang.', 'called', '\"afterglow\"', 'Big', 'Bang,', 'light', 'properly', 'known', 'cosmic', 'microwave', 'background', '(CMB).', 'predicted', 'Ralph', 'Alpher', 'scientists', '1948', 'found', 'accident', '20', 'years', 'later.']]\n",
      "465\n"
     ]
    }
   ],
   "source": [
    "print(text_filtered)\n",
    "print(len(text_filtered[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99970b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each sublist to a string\n",
    "formatted_inputs = [\"summarize: \" + \" \".join(sublist) for sublist in text_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e655c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for formatted_input in formatted_inputs:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate summary using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,\n",
    "        num_beams=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea3d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\", \"astronomers believe universe began 13.7 billion years ago, expanding at unimaginable speed. Big Bang theory predicts uniform background radiation caused high temperatures. existing technology doesn't allow astronomers literally peer universe's birth.\"]\n"
     ]
    }
   ],
   "source": [
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43cc48db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate summary\u001b[39;00m\n\u001b[0;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      5\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2495\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2490\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2491\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2492\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2493\u001b[0m         )\n\u001b[0;32m   2494\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2496\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2497\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2498\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2499\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2500\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2501\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2502\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2503\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2504\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2505\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2506\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2507\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2508\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2509\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2510\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2511\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2512\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2513\u001b[0m     )\n\u001b[0;32m   2514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2516\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2517\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2533\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2534\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2686\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2678\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2679\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2684\u001b[0m )\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2687\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2688\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2689\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2690\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2691\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2692\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2693\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2694\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2695\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2696\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2697\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2698\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2699\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2700\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2701\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2702\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2703\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2704\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils.py:731\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    729\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 731\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m    733\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(ids)\n\u001b[0;32m    734\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text_filtered, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=150,\n",
    "    num_beams=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "\n",
    "# Decode the output\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef00c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract_summary(text, max_length=100):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ddb3a",
   "metadata": {},
   "source": [
    "cosine similarity(v \n",
    "1\n",
    "​\n",
    " ,v \n",
    "2\n",
    "​\n",
    " )= \n",
    "∥v \n",
    "1\n",
    "​\n",
    " ∥∥v \n",
    "2\n",
    "​\n",
    " ∥\n",
    "/(v \n",
    "1\n",
    "​\n",
    " ⋅v \n",
    "2)\n",
    "​\n",
    " \n",
    "​\n",
    "where:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph-based Summarization with TextRank\n",
    "def build_sentence_graph(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    sentence_vectors = [nlp(sent).vector for sent in sentences]\n",
    "    similarity_matrix = np.array([[np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)) for v1 in sentence_vectors] for v2 in sentence_vectors])\n",
    "    \n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    return graph, sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The Transformers library by Hugging Face is a popular library for natural language processing (NLP) tasks. \n",
    "It provides pre-trained models for various tasks such as text generation, translation, summarization, and question answering. \n",
    "The library is built on top of the PyTorch and TensorFlow deep learning frameworks and is known for its user-friendly API and extensive documentation. \n",
    "The models in the Transformers library are state-of-the-art and have been fine-tuned on large datasets, making them highly accurate and efficient for a wide range of NLP applications.\n",
    "\"\"\"\n",
    "\n",
    "graph, sentences = build_sentence_graph(corpus)\n",
    "\n",
    "# Now you can use the graph with a ranking algorithm, e.g., TextRank\n",
    "scores = nx.pagerank(graph)\n",
    "\n",
    "# Sort the sentences by their score\n",
    "ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the top-ranked sentences as the summary\n",
    "summary = \" \".join([s for score, s in ranked_sentences[:3]])\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858264d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_textrank_summary(graph, sentences, top_n=3):\n",
    "    scores = nx.pagerank(graph)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    summary_sentences = [sentences[idx] for idx, _ in sorted_scores[:top_n]]\n",
    "    return \". \".join(summary_sentences) + \".\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_section(tokens, depth, max_depth):\n",
    "    if depth >= max_depth:\n",
    "        return \" \".join([token.text for token in tokens])\n",
    "    \n",
    "    summary = []\n",
    "    for token in tokens:\n",
    "        child_tokens = [child for child in token.children if child in tokens]\n",
    "        if child_tokens:\n",
    "            summary.append(summarize_section(child_tokens, depth + 1, max_depth))\n",
    "        else:\n",
    "            summary.append(token.text)\n",
    "    return \". \".join(summary) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Summarization\n",
    "def generate_hierarchical_summary(text, max_depth=2):\n",
    "    doc = nlp(text)\n",
    "    #return summarize_section(doc.sents, 0, max_depth)\n",
    "    return summarize_section(list(doc), 0, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, sentences = build_sentence_graph(corpus)\n",
    "summary_corpus = extract_textrank_summary(graph, sentences)\n",
    "summary_corpus_1 = generate_hierarchical_summary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph Edges with Weights:\")\n",
    "for u, v, weight in graph.edges(data=True):\n",
    "    print(f\"({u}, {v}, {weight['weight']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe10a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_corpus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Enterprise documents like financial reports, legal contracts, project plans, etc. contain valuable insights and key details related to business operations and decisions. However, these documents tend to be much longer compared to generic corpora. Manually analyzing such long enterprise documents is incredibly time-consuming and labor-intensive. Text summarization methods can help by automatically identifying and extracting the most important information from enterprise documents and generating concise overviews. This provides analysts and decision-makers quick access to the key details without needing to read the full documents. However, standard text summarization techniques often perform poorly on enterprise documents containing unique industry-specific vocabulary and entities not found in their training data. For instance, in credit rating agencies, there are large internal repositories of documents, wikis, and knowledge transfer content developed over many years that are critical for onboarding new analysts. But reading and absorbing all the material is infeasible. Text summarization tailored to such content could help accelerate understanding. Likewise, many large enterprises have company-specific terminologies and named entities which pose challenges for off-the-shelf natural language processing tools. There is a need for customized techniques\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Specify the path to your .docx file\n",
    "docx_file_path = './Text_Summarization/Sample.docx'\n",
    "\n",
    "# Open the .docx file\n",
    "doc = Document(docx_file_path)\n",
    "\n",
    "# Initialize an empty string to store the contents\n",
    "docx_contents = \"\"\n",
    "\n",
    "# Iterate through paragraphs in the document and concatenate them to the string\n",
    "for paragraph in doc.paragraphs:\n",
    "    docx_contents += paragraph.text + \"\\n\"\n",
    "\n",
    "# Print or use the string containing the .docx file contents\n",
    "print(docx_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afa2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"The first stars created bigger atoms and groups of atoms. That led to more stars being born. At the same time, galaxies were crashing and grouping together. As new stars were being born and dying, then things like asteroids, comets, planets, and black holes formed!\"\n",
    "summary = generate_abstract_summary(docx_contents)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76290d",
   "metadata": {},
   "source": [
    "Need to learn\n",
    "\n",
    "1. input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "2. output_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "3. summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "4. similarity_matrix = np.array([[np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)) for v1 in sentence_vectors] for v2 in sentence_vectors])\n",
    "5. graph = nx.from_numpy_array(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d956229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Human machine interface for lab ABC computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user-perceived response time to error measurement\",\n",
    "    \"The generation of random, binary, unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV: Widths of trees and well-quasi-ordering\",\n",
    "    \"Graph minors: A survey\",\n",
    "]\n",
    "\n",
    "# Preprocess documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Train LDA model using LdaMulticore\n",
    "lda_model = LdaMulticore(corpus, num_topics=3, id2word=dictionary, passes=10, workers=2)\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")\n",
    "\n",
    "# Get the topic distribution for the first document\n",
    "doc_topics = lda_model.get_document_topics(corpus[0])\n",
    "print(f\"Topic distribution for the first document: {doc_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model_name = 't5-small'  # You can also use 't5-base', 't5-large', etc.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample input text (a long paragraph)\n",
    "input_text = \"\"\"\n",
    "The Transformers library by Hugging Face is a popular library for natural language processing (NLP) tasks. \n",
    "It provides pre-trained models for various tasks such as text generation, translation, summarization, and question answering. \n",
    "The library is built on top of the PyTorch and TensorFlow deep learning frameworks and is known for its user-friendly API and extensive documentation. \n",
    "The models in the Transformers library are state-of-the-art and have been fine-tuned on large datasets, making them highly accurate and efficient for a wide range of NLP applications.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the input text\n",
    "input_ids = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97229765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
